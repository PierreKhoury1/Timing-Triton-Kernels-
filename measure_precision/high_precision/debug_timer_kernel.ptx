//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_86
.address_size 64

	// .globl	debug_timer_kernel

.visible .entry debug_timer_kernel(
	.param .u64 debug_timer_kernel_param_0,
	.param .u64 debug_timer_kernel_param_1,
	.param .u64 debug_timer_kernel_param_2,
	.param .u64 debug_timer_kernel_param_3
)
.maxntid 32, 1, 1
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<9>;
	.reg .f32 	%f<102>;
	.reg .b64 	%rd<57>;
	.loc	1 30 0
$L__func_begin0:
	.loc	1 30 0

	ld.param.u64 	%rd22, [debug_timer_kernel_param_0];
	ld.param.u64 	%rd23, [debug_timer_kernel_param_1];
$L__tmp0:
	.loc	1 34 24
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	.loc	1 35 17
	shl.b32 	%r5, %r1, 5;
	ld.param.u64 	%rd24, [debug_timer_kernel_param_2];
	ld.param.u64 	%rd25, [debug_timer_kernel_param_3];
	.loc	1 35 38
	mov.u32 	%r6, %tid.x;
	and.b32  	%r7, %r6, 31;
	.loc	1 35 25
	or.b32  	%r8, %r5, %r7;
	.loc	1 36 15
	setp.lt.s32 	%p1, %r8, 32;
$L__tmp1:
	.loc	1 19 8
	// begin inline asm
	mov.u64 %rd1, %globaltimer;
	// end inline asm
$L__tmp2:
	.loc	1 19 8
	// begin inline asm
	mov.u64 %rd2, %globaltimer;
	// end inline asm
$L__tmp3:
	.loc	1 43 21
	sub.s64 	%rd26, %rd2, %rd1;
	.loc	1 45 17
	setp.eq.s64 	%p7, %rd2, %rd1;
	.loc	1 46 46
	min.u64 	%rd27, %rd26, 9223372036854775807;
	selp.b64 	%rd28, 9223372036854775807, %rd27, %p7;
$L__tmp4:
	.loc	1 19 8
	// begin inline asm
	mov.u64 %rd3, %globaltimer;
	// end inline asm
$L__tmp5:
	.loc	1 43 21
	sub.s64 	%rd29, %rd3, %rd2;
	.loc	1 45 17
	setp.eq.s64 	%p8, %rd3, %rd2;
	.loc	1 46 46
	min.u64 	%rd30, %rd29, %rd28;
	selp.b64 	%rd31, %rd28, %rd30, %p8;
$L__tmp6:
	.loc	1 19 8
	// begin inline asm
	mov.u64 %rd4, %globaltimer;
	// end inline asm
$L__tmp7:
	.loc	1 43 21
	sub.s64 	%rd32, %rd4, %rd3;
	.loc	1 45 17
	setp.eq.s64 	%p9, %rd4, %rd3;
	.loc	1 46 46
	min.u64 	%rd33, %rd32, %rd31;
	selp.b64 	%rd34, %rd31, %rd33, %p9;
$L__tmp8:
	.loc	1 19 8
	// begin inline asm
	mov.u64 %rd5, %globaltimer;
	// end inline asm
$L__tmp9:
	.loc	1 43 21
	sub.s64 	%rd35, %rd5, %rd4;
	.loc	1 45 17
	setp.eq.s64 	%p10, %rd5, %rd4;
	.loc	1 46 46
	min.u64 	%rd36, %rd35, %rd34;
	selp.b64 	%rd37, %rd34, %rd36, %p10;
$L__tmp10:
	.loc	1 19 8
	// begin inline asm
	mov.u64 %rd6, %globaltimer;
	// end inline asm
$L__tmp11:
	.loc	1 43 21
	sub.s64 	%rd38, %rd6, %rd5;
	.loc	1 45 17
	setp.eq.s64 	%p11, %rd6, %rd5;
	.loc	1 46 46
	min.u64 	%rd39, %rd38, %rd37;
	selp.b64 	%rd40, %rd37, %rd39, %p11;
$L__tmp12:
	.loc	1 19 8
	// begin inline asm
	mov.u64 %rd7, %globaltimer;
	// end inline asm
$L__tmp13:
	.loc	1 43 21
	sub.s64 	%rd41, %rd7, %rd6;
	.loc	1 45 17
	setp.eq.s64 	%p12, %rd7, %rd6;
	.loc	1 46 46
	min.u64 	%rd42, %rd41, %rd40;
	selp.b64 	%rd43, %rd40, %rd42, %p12;
$L__tmp14:
	.loc	1 19 8
	// begin inline asm
	mov.u64 %rd8, %globaltimer;
	// end inline asm
$L__tmp15:
	.loc	1 43 21
	sub.s64 	%rd44, %rd8, %rd7;
	.loc	1 45 17
	setp.eq.s64 	%p13, %rd8, %rd7;
	.loc	1 46 46
	min.u64 	%rd45, %rd44, %rd43;
	selp.b64 	%rd46, %rd43, %rd45, %p13;
$L__tmp16:
	.loc	1 19 8
	// begin inline asm
	mov.u64 %rd9, %globaltimer;
	// end inline asm
$L__tmp17:
	.loc	1 43 21
	sub.s64 	%rd47, %rd9, %rd8;
	.loc	1 45 17
	setp.eq.s64 	%p14, %rd9, %rd8;
	.loc	1 46 46
	min.u64 	%rd48, %rd47, %rd46;
	selp.b64 	%rd49, %rd46, %rd48, %p14;
$L__tmp18:
	.loc	1 19 8
	// begin inline asm
	mov.u64 %rd10, %globaltimer;
	// end inline asm
$L__tmp19:
	.loc	1 43 21
	sub.s64 	%rd50, %rd10, %rd9;
	.loc	1 45 17
	setp.eq.s64 	%p15, %rd10, %rd9;
	.loc	1 46 46
	min.u64 	%rd51, %rd50, %rd49;
	selp.b64 	%rd52, %rd49, %rd51, %p15;
$L__tmp20:
	.loc	1 19 8
	// begin inline asm
	mov.u64 %rd11, %globaltimer;
	// end inline asm
$L__tmp21:
	.loc	1 43 21
	sub.s64 	%rd53, %rd11, %rd10;
	.loc	1 45 17
	setp.eq.s64 	%p16, %rd11, %rd10;
	.loc	1 46 46
	min.u64 	%rd54, %rd53, %rd52;
	selp.b64 	%rd12, %rd52, %rd54, %p16;
	.loc	1 49 24
	mul.wide.s32 	%rd55, %r8, 8;
	add.s64 	%rd13, %rd25, %rd55;
	.loc	1 49 30
	// begin inline asm
	@%p1 st.global.b64 [ %rd13 + 0 ], { %rd12 };
	// end inline asm
$L__tmp22:
	.loc	1 19 8
	// begin inline asm
	mov.u64 %rd18, %globaltimer;
	// end inline asm
$L__tmp23:
	.loc	1 55 24
	mul.wide.s32 	%rd56, %r8, 4;
	add.s64 	%rd15, %rd22, %rd56;
	mov.b32 	%r3, 0;
	.loc	1 55 16
	// begin inline asm
	mov.u32 %r2, 0x0;
	@%p1 ld.global.b32 { %r2 }, [ %rd15 + 0 ];
	@!%p1 mov.u32 %r2, %r3;
	// end inline asm
	mov.b32 	%f1, %r2;
	.loc	1 57 16
	add.f32 	%f2, %f1, 0f3F800000;
	add.f32 	%f3, %f2, 0f3F800000;
	add.f32 	%f4, %f3, 0f3F800000;
	add.f32 	%f5, %f4, 0f3F800000;
	add.f32 	%f6, %f5, 0f3F800000;
	add.f32 	%f7, %f6, 0f3F800000;
	add.f32 	%f8, %f7, 0f3F800000;
	add.f32 	%f9, %f8, 0f3F800000;
	add.f32 	%f10, %f9, 0f3F800000;
	add.f32 	%f11, %f10, 0f3F800000;
	add.f32 	%f12, %f11, 0f3F800000;
	add.f32 	%f13, %f12, 0f3F800000;
	add.f32 	%f14, %f13, 0f3F800000;
	add.f32 	%f15, %f14, 0f3F800000;
	add.f32 	%f16, %f15, 0f3F800000;
	add.f32 	%f17, %f16, 0f3F800000;
	add.f32 	%f18, %f17, 0f3F800000;
	add.f32 	%f19, %f18, 0f3F800000;
	add.f32 	%f20, %f19, 0f3F800000;
	add.f32 	%f21, %f20, 0f3F800000;
	add.f32 	%f22, %f21, 0f3F800000;
	add.f32 	%f23, %f22, 0f3F800000;
	add.f32 	%f24, %f23, 0f3F800000;
	add.f32 	%f25, %f24, 0f3F800000;
	add.f32 	%f26, %f25, 0f3F800000;
	add.f32 	%f27, %f26, 0f3F800000;
	add.f32 	%f28, %f27, 0f3F800000;
	add.f32 	%f29, %f28, 0f3F800000;
	add.f32 	%f30, %f29, 0f3F800000;
	add.f32 	%f31, %f30, 0f3F800000;
	add.f32 	%f32, %f31, 0f3F800000;
	add.f32 	%f33, %f32, 0f3F800000;
	add.f32 	%f34, %f33, 0f3F800000;
	add.f32 	%f35, %f34, 0f3F800000;
	add.f32 	%f36, %f35, 0f3F800000;
	add.f32 	%f37, %f36, 0f3F800000;
	add.f32 	%f38, %f37, 0f3F800000;
	add.f32 	%f39, %f38, 0f3F800000;
	add.f32 	%f40, %f39, 0f3F800000;
	add.f32 	%f41, %f40, 0f3F800000;
	add.f32 	%f42, %f41, 0f3F800000;
	add.f32 	%f43, %f42, 0f3F800000;
	add.f32 	%f44, %f43, 0f3F800000;
	add.f32 	%f45, %f44, 0f3F800000;
	add.f32 	%f46, %f45, 0f3F800000;
	add.f32 	%f47, %f46, 0f3F800000;
	add.f32 	%f48, %f47, 0f3F800000;
	add.f32 	%f49, %f48, 0f3F800000;
	add.f32 	%f50, %f49, 0f3F800000;
	add.f32 	%f51, %f50, 0f3F800000;
	add.f32 	%f52, %f51, 0f3F800000;
	add.f32 	%f53, %f52, 0f3F800000;
	add.f32 	%f54, %f53, 0f3F800000;
	add.f32 	%f55, %f54, 0f3F800000;
	add.f32 	%f56, %f55, 0f3F800000;
	add.f32 	%f57, %f56, 0f3F800000;
	add.f32 	%f58, %f57, 0f3F800000;
	add.f32 	%f59, %f58, 0f3F800000;
	add.f32 	%f60, %f59, 0f3F800000;
	add.f32 	%f61, %f60, 0f3F800000;
	add.f32 	%f62, %f61, 0f3F800000;
	add.f32 	%f63, %f62, 0f3F800000;
	add.f32 	%f64, %f63, 0f3F800000;
	add.f32 	%f65, %f64, 0f3F800000;
	add.f32 	%f66, %f65, 0f3F800000;
	add.f32 	%f67, %f66, 0f3F800000;
	add.f32 	%f68, %f67, 0f3F800000;
	add.f32 	%f69, %f68, 0f3F800000;
	add.f32 	%f70, %f69, 0f3F800000;
	add.f32 	%f71, %f70, 0f3F800000;
	add.f32 	%f72, %f71, 0f3F800000;
	add.f32 	%f73, %f72, 0f3F800000;
	add.f32 	%f74, %f73, 0f3F800000;
	add.f32 	%f75, %f74, 0f3F800000;
	add.f32 	%f76, %f75, 0f3F800000;
	add.f32 	%f77, %f76, 0f3F800000;
	add.f32 	%f78, %f77, 0f3F800000;
	add.f32 	%f79, %f78, 0f3F800000;
	add.f32 	%f80, %f79, 0f3F800000;
	add.f32 	%f81, %f80, 0f3F800000;
	add.f32 	%f82, %f81, 0f3F800000;
	add.f32 	%f83, %f82, 0f3F800000;
	add.f32 	%f84, %f83, 0f3F800000;
	add.f32 	%f85, %f84, 0f3F800000;
	add.f32 	%f86, %f85, 0f3F800000;
	add.f32 	%f87, %f86, 0f3F800000;
	add.f32 	%f88, %f87, 0f3F800000;
	add.f32 	%f89, %f88, 0f3F800000;
	add.f32 	%f90, %f89, 0f3F800000;
	add.f32 	%f91, %f90, 0f3F800000;
	add.f32 	%f92, %f91, 0f3F800000;
	add.f32 	%f93, %f92, 0f3F800000;
	add.f32 	%f94, %f93, 0f3F800000;
	add.f32 	%f95, %f94, 0f3F800000;
	add.f32 	%f96, %f95, 0f3F800000;
	add.f32 	%f97, %f96, 0f3F800000;
	add.f32 	%f98, %f97, 0f3F800000;
	add.f32 	%f99, %f98, 0f3F800000;
	add.f32 	%f100, %f99, 0f3F800000;
	add.f32 	%f101, %f100, 0f3F800000;
	.loc	1 58 27
	mov.b32 	%r4, %f101;
	// begin inline asm
	@%p1 st.global.b32 [ %rd15 + 0 ], { %r4 };
	// end inline asm
$L__tmp24:
	.loc	1 19 8
	// begin inline asm
	mov.u64 %rd20, %globaltimer;
	// end inline asm
$L__tmp25:
	.loc	1 65 22
	add.s64 	%rd19, %rd23, %rd55;
	.loc	1 65 28
	// begin inline asm
	@%p1 st.global.b64 [ %rd19 + 0 ], { %rd18 };
	// end inline asm
	.loc	1 66 22
	add.s64 	%rd21, %rd24, %rd55;
	.loc	1 66 28
	// begin inline asm
	@%p1 st.global.b64 [ %rd21 + 0 ], { %rd20 };
	// end inline asm
	.loc	1 66 4
	ret;
$L__tmp26:
$L__func_end0:

}
	.file	1 "/home/pierreisnotrock/Documents/triton_testing/triton_test1/minimal_test6.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 11
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 254
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 109
.b8 105
.b8 110
.b8 105
.b8 109
.b8 97
.b8 108
.b8 95
.b8 116
.b8 101
.b8 115
.b8 116
.b8 54
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 104
.b8 111
.b8 109
.b8 101
.b8 47
.b8 112
.b8 105
.b8 101
.b8 114
.b8 114
.b8 101
.b8 105
.b8 115
.b8 110
.b8 111
.b8 116
.b8 114
.b8 111
.b8 99
.b8 107
.b8 47
.b8 68
.b8 111
.b8 99
.b8 117
.b8 109
.b8 101
.b8 110
.b8 116
.b8 115
.b8 47
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 95
.b8 116
.b8 101
.b8 115
.b8 116
.b8 105
.b8 110
.b8 103
.b8 47
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 95
.b8 116
.b8 101
.b8 115
.b8 116
.b8 49
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 100
.b8 101
.b8 98
.b8 117
.b8 103
.b8 95
.b8 116
.b8 105
.b8 109
.b8 101
.b8 114
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 118
.b8 4
.b32 118
.b64 $L__tmp1
.b64 $L__tmp2
.b8 1
.b8 39
.b8 13
.b8 4
.b32 118
.b64 $L__tmp2
.b64 $L__tmp21
.b8 1
.b8 42
.b8 16
.b8 4
.b32 118
.b64 $L__tmp22
.b64 $L__tmp23
.b8 1
.b8 52
.b8 9
.b8 4
.b32 118
.b64 $L__tmp24
.b64 $L__tmp25
.b8 1
.b8 62
.b8 9
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}
